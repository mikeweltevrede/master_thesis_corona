{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, glob, os\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "keep_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eurostat_reader(file_path, na_proportion = 0.8, \n",
    "                    cols_to_drop={\"all\": {'UNIT'},\n",
    "                                  \"arrivals_at_tourist_accommodation_establishments.zip\" : {'C_RESID'},\n",
    "                                  \"average_length_of_stay_at_hospitals.zip\": {'SEX', 'AGE', 'ICD10'},\n",
    "                                  \"crude_death_rate.zip\": {'SEX', 'AGE'},\n",
    "                                  \"disposable_income_per_inhabitant.zip\": {'DIRECT'},\n",
    "                                  \"percentage_education_attainment.zip\": {'SEX', 'AGE'},\n",
    "                                  \"population_numbers.zip\": {'SEX', 'AGE'}}):\n",
    "\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "        for file in zip_file.namelist():\n",
    "            if 'Data' in file:\n",
    "                with zip_file.open(file) as data_file:\n",
    "                    df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "    \n",
    "    base_file_path = os.path.basename(file_path)        \n",
    "    # Drop desired columns\n",
    "    if base_file_path in cols_to_drop.keys():\n",
    "        if (len(cols_to_drop[base_file_path].intersection(df.columns)) > 0):\n",
    "            df = df.drop(columns=cols_to_drop[base_file_path].intersection(df.columns))\n",
    "    df = df.drop(columns=cols_to_drop[\"all\"])\n",
    "        \n",
    "    # Clean Value column and convert to float\n",
    "    df[\"Value\"] = df[\"Value\"].apply(\n",
    "        lambda x: x.replace(\",\", \"\")).replace({\":\": None}, regex=False).astype(np.float32)\n",
    "    \n",
    "    # Check if there is no data for certain years and then drop those years\n",
    "    na_check = df.set_index('TIME')['Value'].isna().all(level=0)\n",
    "    if len(na_check.index[na_check]) > 0:\n",
    "        df = df.set_index('TIME').drop(list(na_check.index[na_check])).reset_index()\n",
    "    \n",
    "    # Find the column containing the relevant values to spread on\n",
    "    value_col = [col for col in df.columns if col not in {\"TIME\", \"GEO\", \"Value\"}]\n",
    "    \n",
    "    if len(value_col) > 1:\n",
    "        raise ValueError(f\"Too many columns available to spread on for file '{file_path}', \"\n",
    "                         f\"namely {value_col}. Check the data and add columns to remove to cols_to_drop.\")  \n",
    "    \n",
    "    # Pivot from long to wide\n",
    "    if len(value_col) == 1:\n",
    "        df = df.pivot_table(index=['TIME','GEO'], columns=value_col[0], values='Value').reset_index()\n",
    "        try:\n",
    "            del df.columns.name\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    # Select only the latest data for which all data is known\n",
    "    test = (df.groupby('TIME').apply(lambda x: len(x) - x.isna().sum()) > 0).all(axis=1)\n",
    "    max_year = test[test].index.max()\n",
    "    df = df.set_index(\"TIME\").loc[max_year].reset_index()\n",
    "    \n",
    "    # Drop fully NA rows and columns\n",
    "    df = df.dropna(how='all').dropna(axis=1, how='all')\n",
    "    na_props = df.isna().sum().divide(df.apply(len))\n",
    "    \n",
    "    if len(na_props[na_props > na_proportion].index) > 0:\n",
    "        print(f\"Over {na_proportion*100} percent NAs in the column(s) \"\n",
    "              f\"{na_props[na_props > na_proportion].index} in file {base_file_path}. \"\n",
    "              \"Dropping these columns\")\n",
    "    \n",
    "        df = df.drop(columns=na_props[na_props > na_proportion].index)\n",
    "    \n",
    "    if len(value_col) == 0:\n",
    "        df = df.rename(columns={'Value': base_file_path.replace('.zip', '')})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 80.0 percent NAs in the column(s) Index(['Navigable canals', 'Navigable rivers'], dtype='object') in file length_of_navigable_roads.zip. Dropping these columns\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in glob.glob(\"data/eurostat/*.zip\"):\n",
    "    if 'by_rail_by_loading_unloading_region' in file:\n",
    "        continue\n",
    "    if 'TOCLEAN' in file:\n",
    "        # TODO: Skip these files for now. These are per NUTS3 region and need to be aggregated.\n",
    "        continue\n",
    "    \n",
    "    df = eurostat_reader(file)\n",
    "    \n",
    "    if keep_time:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        dfs.append(df.drop(columns=[\"TIME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge (outer join) the list of DataFrames into one big DataFrame\n",
    "if keep_time:\n",
    "    df_merged = reduce(lambda x, y: pd.merge(x, y, on=['TIME', 'GEO'], how='outer'),\n",
    "                       dfs).sort_values(by=['TIME', 'GEO'])\n",
    "else:\n",
    "    df_merged = reduce(lambda x, y: pd.merge(x, y, on=['GEO'], how='outer'),\n",
    "                       dfs).sort_values(by=['GEO'])\n",
    "\n",
    "# Drop extra regions\n",
    "extra_region = (df_merged[\"GEO\"] == \"Extra-Regio NUTS 1\") | (df_merged[\"GEO\"] == \"Extra-Regio NUTS 2\")\n",
    "df_merged = df_merged.drop(extra_region[extra_region].index)\n",
    "\n",
    "# Propagate constant values over years for each region - Total area\n",
    "if keep_time:\n",
    "    df_merged = df_merged.set_index('GEO')\n",
    "    constant_columns = ['Total area']\n",
    "\n",
    "    for region in df_merged.index.unique():\n",
    "        for col in constant_columns:\n",
    "            value = df_merged.loc[region, col][df_merged.loc[region, col].notna()]\n",
    "\n",
    "            try:\n",
    "                df_merged.loc[region, col] = [value for i in range(len(df_merged.loc[region, col]))]\n",
    "            except ValueError:\n",
    "                print(f\"Region: {region}, Value: {value}\")\n",
    "\n",
    "    df_merged = df_merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename = {\n",
    "    'GEO': 'region',\n",
    "    'Freight and mail loaded': 'air_freight_loaded',\n",
    "    'Freight and mail unloaded': 'air_freight_unloaded',\n",
    "    'Passengers carried (arrival)': 'air_passengers_arrived',\n",
    "    'Passengers carried (departures)': 'air_passengers_departed',\n",
    "    'Total area': 'area',\n",
    "    'Hotels; holiday and other short-stay accommodation; camping grounds, recreational vehicle parks and trailer parks': 'tourist_arrivals',\n",
    "    'In-patient average length of stay (in days)': 'hospital_stay',\n",
    "    'All causes of death (A00-Y89) excluding S00-T98': 'death_rate_all',\n",
    "    'Human immunodeficiency virus [HIV] disease': 'death_rate_hiv',\n",
    "    'Influenza (including swine flu)': 'death_rate_influenza',\n",
    "    'Tuberculosis': 'death_rate_tb',\n",
    "    'Viral hepatitis and sequelae of viral hepatitis': 'death_rate_viralhepatitis',\n",
    "    'Disposable income, net': 'disposable_income',\n",
    "    'Medical doctors': 'medical_doctors',\n",
    "    'Nurses and midwives': 'nurses_midwives',\n",
    "    'Available beds in hospitals (HP.1)': 'available_beds',\n",
    "    'Curative care beds in hospitals (HP.1)': 'curative_care_beds',\n",
    "    'Long-term care beds in hospitals (HP.1)': 'longterm_care_beds',\n",
    "    'Other beds in hospitals (HP.1)': 'other_beds',\n",
    "    'Psychiatric care beds in hospitals (HP.1)': 'psychiatric_care_beds',\n",
    "    'Rehabilitative care beds in hospitals (HP.1)': 'rehabilitative_care_beds',\n",
    "    'Internet use: interaction with public authorities (last 12 months)': 'internet_contact_authorities',   \n",
    "    'Electrified railway lines': 'length_electrified_railway',\n",
    "    'Motorways': 'length_motorways',\n",
    "    'Navigable canals': 'length_canals',\n",
    "    'Navigable rivers': 'length_rivers',\n",
    "    'Other roads': 'length_other_roads',\n",
    "    'Railway lines with double and more tracks': 'length_large_railway',\n",
    "    'Total railway lines': 'length_railway',\n",
    "    'Freight loaded': 'maritime_freight_loaded',\n",
    "    'Freight unloaded': 'maritime_freight_unloaded',\n",
    "    'Passengers disembarked': 'maritime_passengers_disembarked',\n",
    "    'Passengers embarked': 'maritime_passengers_embarked',\n",
    "    'Median age of population': 'median_age',\n",
    "    'Less than primary, primary and lower secondary education (levels 0-2)': 'lower_education',\n",
    "    'Upper secondary and post-secondary non-tertiary education (levels 3 and 4)': 'higher_education',\n",
    "    'Tertiary education (levels 5-8)': 'tertiary_education'\n",
    "}\n",
    "\n",
    "if keep_time:\n",
    "    dict_rename['TIME'] = 'time'\n",
    "\n",
    "df_merged = df_merged.rename(columns=dict_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"data/merged_eurostat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-city railroad connections\n",
    "# file_path = (\"data/eurostat/passengers_by_rail_by_loading_unloading_region.zip\")\n",
    "\n",
    "# with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "#     for file in zip_file.namelist():\n",
    "#         if 'Data' in file:\n",
    "#             with zip_file.open(file) as data_file:\n",
    "#                 df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# # GEO is constant, simply \"Italy\"\n",
    "# df = df.drop(columns=['UNIT', 'GEO'])\n",
    "\n",
    "# # Clean Value column and convert to float\n",
    "# df[\"Value\"] = df[\"Value\"].apply(\n",
    "#     lambda x: x.replace(\",\", \"\")).replace({\":\": None}, regex=False).astype(np.float32)\n",
    "\n",
    "# # Check if there is no data for certain years and then drop those years\n",
    "# na_check = df.set_index('TIME')['Value'].isna().all(level=0)\n",
    "# if len(na_check.index[na_check]) > 0:\n",
    "#     df = df.set_index('TIME').drop(list(na_check.index[na_check])).reset_index()\n",
    "\n",
    "# # Pivot from long to wide\n",
    "# df = df.pivot_table('Value', index=['TIME', 'C_LOAD'], columns='C_UNLOAD').reset_index()\n",
    "# del df.columns.name\n",
    "\n",
    "# df.to_csv(\"data/interregion_railroad_travel.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
